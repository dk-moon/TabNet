{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error, mean_absolute_error, mean_squared_error\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial, visualization\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.augmentations import RegressionSMOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv()\n",
    "test = pd.read_csv()\n",
    "submission = pd.read_csv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SPP(df):\n",
    "    n_unique = df.nunique() # 각 Column에서의 Unique한 Value의 수\n",
    "    types = df.dtypes\n",
    "    threshold = 10 # Categorical Feature가 가질 Unique한 Value의 가지 수 제한\n",
    "    \n",
    "    cat_columns = [] # Categorical 컬럼을 담을 리스트\n",
    "    cat_dims = {} # Categorical 컬럼과 Unique한 Value를 담을 딕셔너리\n",
    "    \n",
    "    for col in tqdm(df.columns):\n",
    "        print(col, df[col].nunique())\n",
    "        if types[col] == 'object' or n_unique[col] < threshold:\n",
    "            l_enc = LabelEncoder()\n",
    "            df[col] = df[col].fillna(\"NULL\") # 결측치를 \"NULL\"이라는 문자열로 치환\n",
    "            df[col] = l_enc.fit_transform(df[col].values)\n",
    "            cat_columns.append(col)\n",
    "            cat_dims[col] = len(l_enc.classes_)\n",
    "        else:\n",
    "            df.fillna(df[col].mean(), inplace=True)\n",
    "    return cat_columns, cat_dims, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns, cat_dims, train = SPP(train)\n",
    "test = SPP(test)[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Categorical Features for Categorical embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"\" # Target Feature\n",
    "unused_feat = [] # 학습 시 제외할 column (ex. ID)\n",
    "\n",
    "features = [ col for col in train.columns if col not in unused_feat+[target]] \n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in cat_columns]\n",
    "cat_dims = [ cat_dims[f] for i, f in enumerate(features) if f in cat_columns]\n",
    "cat_emb_dim = [5, 4, 3, 6, 2, 2, 1] # Random하게 지정?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train.iloc[:,:-1],train.iloc[:,-1],\n",
    "    test_size=0.3,\n",
    "    random_state=530,\n",
    "    shuffle=True,\n",
    "    stratify=train.iloc[:,-1]\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train,y_train,\n",
    "    test_size=0.3,\n",
    "    random_state=530,\n",
    "    shuffle=True,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "# 학습용\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# 확인용\n",
    "X_valid = X_valid.to_numpy()\n",
    "y_valid = y_valid.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# 검증용\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Objective(trial):\n",
    "    mask_type = trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"])\n",
    "    n_da = trial.suggest_int(\"n_da\", 32, 128, step=4)\n",
    "    n_steps = trial.suggest_int(\"n_steps\", 1, 5, step=1)\n",
    "    gamma = trial.suggest_float(\"gamma\", 1., 1.4, step=0.2)\n",
    "    n_shared = trial.suggest_int(\"n_shared\", 1, 3, step=1)\n",
    "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n",
    "    tabnet_params = dict(n_d=n_da, n_a=n_da, n_steps=n_steps, gamma=gamma,\n",
    "                     lambda_sparse=lambda_sparse, optimizer_fn=torch.optim.Adam,\n",
    "                     optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "                     mask_type=mask_type, n_shared=n_shared,\n",
    "                     scheduler_params=dict(mode=\"min\",\n",
    "                                           patience=trial.suggest_int(\"patienceScheduler\",low=30,high=50), # changing sheduler patience to be lower than early stopping patience \n",
    "                                           min_lr=1e-5,\n",
    "                                           factor=0.5,),\n",
    "                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau) #early stopping\n",
    "    \n",
    "    aug = RegressionSMOTE(p=0.2)\n",
    "    \n",
    "    regressor = TabNetRegressor(**tabnet_params)\n",
    "    regressor.fit(X_train=X_train, y_train=y_train,\n",
    "                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_name=['train', 'valid'],\n",
    "                  eval_metric=['rmsle', 'mae', 'rmse', 'mse'],\n",
    "                  patience=trial.suggest_int(\"patience\",low=30,high=50), max_epochs=trial.suggest_int('epochs', 50, 100),\n",
    "                  batch_size=1024, virtual_batch_size=128,\n",
    "                  drop_last=False,\n",
    "                  augmentations=aug, #aug, None\n",
    "                 )\n",
    "    best_cost = regressor.best_cost\n",
    "    \n",
    "    return best_cost\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", study_name='TabNet optimization')\n",
    "study.optimize(Objective, timeout=6*60) #5 hours"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a TabNet with the best params to make submission\n",
    "TabNet_params = study.best_params\n",
    "\n",
    "print(TabNet_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = dict(n_d=TabNet_params['n_da'], n_a=TabNet_params['n_da'],\n",
    "                    n_steps=TabNet_params['n_steps'], gamma=TabNet_params['gamma'],\n",
    "                    lambda_sparse=TabNet_params['lambda_sparse'],\n",
    "                    optimizer_fn=torch.optim.Adam,\n",
    "                    optimizer_params=dict(lr=2e-2, weight_decay=1e-5),\n",
    "                    mask_type=TabNet_params['mask_type'],\n",
    "                    n_shared=TabNet_params['n_shared'],\n",
    "                    scheduler_params=dict(mode=\"min\",\n",
    "                                          patience=TabNet_params['patienceScheduler'],\n",
    "                                          min_lr=1e-5,\n",
    "                                          factor=0.5,),\n",
    "                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "                     )\n",
    "epochs = TabNet_params['epochs']\n",
    "\n",
    "aug = RegressionSMOTE(p=0.2)\n",
    "\n",
    "regressor = TabNetRegressor(**final_params)\n",
    "regressor.fit(X_train=X_train, y_train=y_train,\n",
    "              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              eval_name=['train', 'valid'],\n",
    "              eval_metric=['rmsle', 'mae', 'rmse', 'mse'],\n",
    "              patience=TabNet_params['patience'], max_epochs=epochs,\n",
    "              batch_size=1024, virtual_batch_size=128,\n",
    "              drop_last=False,\n",
    "              augmentations=aug, #aug, None\n",
    "             )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Result Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.plot(regressor.history['loss'])\n",
    "plt.show()\n",
    "\n",
    "# plot rmse\n",
    "plt.plot(regressor.history['train_rmse'])\n",
    "plt.plot(regressor.history['valid_rmse'])\n",
    "plt.show()\n",
    "\n",
    "# plot learning rates\n",
    "plt.plot(regressor.history['lr'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = regressor.predict(X_test)\n",
    "\n",
    "test_mae = mean_absolute_error(y_true=y_test,y_pred=preds[:,])\n",
    "test_mse = mean_squared_error(y_true=y_test,y_pred=preds[:,])\n",
    "test_msle = mean_squared_log_error(y_true=y_test,y_pred=preds[:,])\n",
    "\n",
    "print(f\"BEST VALID SCORE : {regressor.best_cost}\")\n",
    "print(f\"FINAL TEST MAE : {test_mae}\")\n",
    "print(f\"FINAL TEST MSE : {test_mse}\")\n",
    "print(f\"FINAL TEST MSLE : {test_msle}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local explainability and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_matrix, masks = regressor.explain(X_test)\n",
    "\n",
    "print(f\"Mask Lenght : {len(masks)}\")\n",
    "\n",
    "mask_agg = []\n",
    "for i in tqdm(range(len(masks))):\n",
    "    mask_df = pd.DataFrame(data=masks[i],columns=features[:])\n",
    "    \n",
    "    # 각 Mask의 Columns의 중요도의 합\n",
    "    col_sums = []\n",
    "    for j in range(len(mask_df.columns)):\n",
    "        sums = sum(mask_df.iloc[:,j].values)\n",
    "        col_sums.append(sums)\n",
    "    \n",
    "    plt.title(f\"Step {i} Importance Bar Plot\")\n",
    "    plt.bar(mask_df.columns, col_sums)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "    # 모든 Mask의 합\n",
    "    for j in range(len(mask_df)):\n",
    "        vals = list(mask_df[j].values)\n",
    "        mask_agg.append(vals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global explainability : feature importance summing to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_exp = regressor.feature_importances_\n",
    "print(global_exp)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.bar(features[:],global_exp)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_X_test = test.to_numpy()\n",
    "pred_y_test = regressor.predict(pred_X_test)\n",
    "submission[target] = pred_y_test\n",
    "\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
